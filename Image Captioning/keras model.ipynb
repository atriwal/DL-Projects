{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "filename = 'input/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n"
     ]
    }
   ],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\t# process lines\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\t# take the first token as the image id, the rest as the description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# remove filename from image id\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\t# convert description tokens back to string\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\t# store the first description for each image\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = image_desc\n",
    "\treturn mapping\n",
    " \n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 4484\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def clean_descriptions(descriptions):\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc in descriptions.items():\n",
    "\t\t# tokenize\n",
    "\t\tdesc = desc.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t# remove hanging 's' and 'a'\n",
    "\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t# store as string\n",
    "\t\tdescriptions[key] =  ' '.join(desc)\n",
    " \n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "all_tokens = ' '.join(descriptions.values()).split()\n",
    "vocabulary = set(all_tokens)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc in descriptions.items():\n",
    "\t\tlines.append(key + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# save descriptions\n",
    "save_doc(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed : 0.2343436082204183 mins\n",
      ">3319058642_885d756295.jpg\n",
      "100\n",
      "time elapsed : 0.4499853452046712 mins\n",
      ">3172384527_b107385a20.jpg\n",
      "200\n",
      "time elapsed : 0.6662875533103942 mins\n",
      ">2347921097_f2e35753c0.jpg\n",
      "300\n",
      "time elapsed : 0.8826464573542278 mins\n",
      ">2097403787_77a154f5b9.jpg\n",
      "400\n",
      "time elapsed : 1.103163210550944 mins\n",
      ">2943384009_c8cf749181.jpg\n",
      "500\n",
      "time elapsed : 1.3255232294400534 mins\n",
      ">3668518431_43abb169eb.jpg\n",
      "600\n",
      "time elapsed : 1.543960444132487 mins\n",
      ">1579198375_84b18e003a.jpg\n",
      "700\n",
      "time elapsed : 1.7603347460428873 mins\n",
      ">506808265_fe84ada926.jpg\n",
      "800\n",
      "time elapsed : 1.9769954601923625 mins\n",
      ">2393196444_8f4f540f5f.jpg\n",
      "900\n",
      "time elapsed : 2.1931206425031027 mins\n",
      ">339658315_fbb178c252.jpg\n",
      "1000\n",
      "time elapsed : 2.412825512886047 mins\n",
      ">3183883750_b6acc40397.jpg\n",
      "1100\n",
      "time elapsed : 2.6310842196146647 mins\n",
      ">3450776690_38605c667d.jpg\n",
      "1200\n",
      "time elapsed : 2.8489818930625916 mins\n",
      ">3156113206_53c2a7b5d8.jpg\n",
      "1300\n",
      "time elapsed : 3.0673654238382975 mins\n",
      ">2369248869_0266760c4a.jpg\n",
      "1400\n",
      "time elapsed : 3.2867521325747173 mins\n",
      ">2124040721_bffc0a091a.jpg\n",
      "1500\n",
      "time elapsed : 3.5055236419041953 mins\n",
      ">472860064_a96a228796.jpg\n",
      "1600\n",
      "time elapsed : 3.7241124232610066 mins\n",
      ">3109136206_f7d201b368.jpg\n",
      "1700\n",
      "time elapsed : 3.942777069409688 mins\n",
      ">2203449950_e51d0f9065.jpg\n",
      "1800\n",
      "time elapsed : 4.160994990666707 mins\n",
      ">2713554148_64cd465e71.jpg\n",
      "1900\n",
      "time elapsed : 4.379438920815786 mins\n",
      ">2335428699_4eba9b6245.jpg\n",
      "2000\n",
      "time elapsed : 4.597495722770691 mins\n",
      ">1453366750_6e8cf601bf.jpg\n",
      "2100\n",
      "time elapsed : 4.819278172651926 mins\n",
      ">2445442929_8c81d42460.jpg\n",
      "2200\n",
      "time elapsed : 5.134345209598541 mins\n",
      ">2787276494_82703f570a.jpg\n",
      "2300\n",
      "time elapsed : 5.432711009184519 mins\n",
      ">3282634762_2650d0088a.jpg\n",
      "2400\n",
      "time elapsed : 5.699145193894704 mins\n",
      ">2860041212_797afd6ccf.jpg\n",
      "2500\n",
      "time elapsed : 5.963882660865783 mins\n",
      ">1383840121_c092110917.jpg\n",
      "2600\n",
      "time elapsed : 6.225583636760712 mins\n",
      ">535529555_583d89b7f2.jpg\n",
      "2700\n",
      "time elapsed : 6.450394610563914 mins\n",
      ">3220126881_b0a4f7cccb.jpg\n",
      "2800\n",
      "time elapsed : 6.6719484011332195 mins\n",
      ">2559638792_a803ff63d1.jpg\n",
      "2900\n",
      "time elapsed : 6.892450718084971 mins\n",
      ">3673484638_dce87295fe.jpg\n",
      "3000\n",
      "time elapsed : 7.1467520833015445 mins\n",
      ">3045613316_4e88862836.jpg\n",
      "3100\n",
      "time elapsed : 7.413803764184316 mins\n",
      ">2520909293_9bb7f7364e.jpg\n",
      "3200\n",
      "time elapsed : 7.6777771472930905 mins\n",
      ">429270993_294ba8e64c.jpg\n",
      "3300\n",
      "time elapsed : 7.942011984189351 mins\n",
      ">3319405494_58dee86b21.jpg\n",
      "3400\n",
      "time elapsed : 8.206457308928172 mins\n",
      ">2460159430_71ab1aacfa.jpg\n",
      "3500\n",
      "time elapsed : 8.470146822929383 mins\n",
      ">2110692070_8aaaa1ae39.jpg\n",
      "3600\n",
      "time elapsed : 8.732457343737284 mins\n",
      ">1248734482_3038218f3b.jpg\n",
      "3700\n",
      "time elapsed : 8.995792043209075 mins\n",
      ">2752341621_54490b9b09.jpg\n",
      "3800\n",
      "time elapsed : 9.25942484140396 mins\n",
      ">3136043366_b3f8607a0e.jpg\n",
      "3900\n",
      "time elapsed : 9.522075772285461 mins\n",
      ">2574509968_e4692ae169.jpg\n",
      "4000\n",
      "time elapsed : 9.78439482053121 mins\n",
      ">2271264741_aa8f73f87c.jpg\n",
      "4100\n",
      "time elapsed : 10.068508780002594 mins\n",
      ">1536597926_c2e1bc2379.jpg\n",
      "4200\n",
      "time elapsed : 10.451493155956268 mins\n",
      ">3016521240_2ef20834b6.jpg\n",
      "4300\n",
      "time elapsed : 10.688850398858389 mins\n",
      ">241347635_e691395c2f.jpg\n",
      "4400\n",
      "time elapsed : 10.907170335451761 mins\n",
      ">2656749876_e32495bd8c.jpg\n",
      "4500\n",
      "time elapsed : 11.127345351378123 mins\n",
      ">2993388841_6746140656.jpg\n",
      "4600\n",
      "time elapsed : 11.347250763575236 mins\n",
      ">3672057606_cb6393dbd9.jpg\n",
      "4700\n",
      "time elapsed : 11.628997429211934 mins\n",
      ">3006095077_1992b677f8.jpg\n",
      "4800\n",
      "time elapsed : 11.854098415374756 mins\n",
      ">517094985_4b9e926936.jpg\n",
      "4900\n",
      "time elapsed : 12.07477127313614 mins\n",
      ">2431120202_b24fe2333a.jpg\n",
      "5000\n",
      "time elapsed : 12.296023925145468 mins\n",
      ">2708744743_e231f7fcf9.jpg\n",
      "5100\n",
      "time elapsed : 12.517195641994476 mins\n",
      ">2573667207_a1bf49befc.jpg\n",
      "5200\n",
      "time elapsed : 12.741906909147898 mins\n",
      ">2718049631_e7aa74cb9b.jpg\n",
      "5300\n",
      "time elapsed : 12.962451605002085 mins\n",
      ">3003691049_f4363c2d5c.jpg\n",
      "5400\n",
      "time elapsed : 13.19471486012141 mins\n",
      ">1982852140_56425fa7a2.jpg\n",
      "5500\n",
      "time elapsed : 13.412422549724578 mins\n",
      ">2816259113_461f8dedb0.jpg\n",
      "5600\n",
      "time elapsed : 13.628910728295644 mins\n",
      ">2269021076_cefc9af989.jpg\n",
      "5700\n",
      "time elapsed : 13.845540245374044 mins\n",
      ">2596100297_372bd0f739.jpg\n",
      "5800\n",
      "time elapsed : 14.062010085582733 mins\n",
      ">3187924573_203223e6c0.jpg\n",
      "5900\n",
      "time elapsed : 14.278786389033 mins\n",
      ">2904997007_23d4b94101.jpg\n",
      "6000\n",
      "time elapsed : 14.495298266410828 mins\n",
      ">3180806542_49b6de312d.jpg\n",
      "6100\n",
      "time elapsed : 14.711282722155254 mins\n",
      ">3398276602_c7d106c34f.jpg\n",
      "6200\n",
      "time elapsed : 14.927775402863821 mins\n",
      ">2890113532_ab2003d74e.jpg\n",
      "6300\n",
      "time elapsed : 15.144288774331411 mins\n",
      ">2934801096_230ae78d7e.jpg\n",
      "6400\n",
      "time elapsed : 15.36216115951538 mins\n",
      ">3443030942_f409586258.jpg\n",
      "6500\n",
      "time elapsed : 15.578555969397227 mins\n",
      ">3482062809_3b694322c4.jpg\n",
      "6600\n",
      "time elapsed : 15.794940376281739 mins\n",
      ">3558370311_5734a15890.jpg\n",
      "6700\n",
      "time elapsed : 16.01169867912928 mins\n",
      ">3578914491_36019ba703.jpg\n",
      "6800\n",
      "time elapsed : 16.22791951497396 mins\n",
      ">2380464803_a64f05bfa9.jpg\n",
      "6900\n",
      "time elapsed : 16.44411677122116 mins\n",
      ">3286193613_fc046e8016.jpg\n",
      "7000\n",
      "time elapsed : 16.66364196141561 mins\n",
      ">3028561714_83fb921067.jpg\n",
      "7100\n",
      "time elapsed : 16.88049476146698 mins\n",
      ">3241892328_4ebf8b21ce.jpg\n",
      "7200\n",
      "time elapsed : 17.097278559207915 mins\n",
      ">3215117062_6e07a86352.jpg\n",
      "7300\n",
      "time elapsed : 17.31543353398641 mins\n",
      ">2393298349_e659308218.jpg\n",
      "7400\n",
      "time elapsed : 17.531176642576852 mins\n",
      ">967719295_3257695095.jpg\n",
      "7500\n",
      "time elapsed : 17.747377610206605 mins\n",
      ">3062273350_fd66106f21.jpg\n",
      "7600\n",
      "time elapsed : 17.964613469441733 mins\n",
      ">3318564834_4ccea90497.jpg\n",
      "7700\n",
      "time elapsed : 18.18121172984441 mins\n",
      ">1460352062_d64fb633e0.jpg\n",
      "7800\n",
      "time elapsed : 18.397397391001384 mins\n",
      ">3456362961_d8f7e347a8.jpg\n",
      "7900\n",
      "time elapsed : 18.61370857556661 mins\n",
      ">3212456649_40a3052682.jpg\n",
      "8000\n",
      "Extracted Features: 8091\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.layers import Input\n",
    "import time\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = MobileNet(include_top=False, input_tensor=in_layer,input_shape=(224,224,3))\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        counter = counter + 1\n",
    "        if(counter%100 == 0):\n",
    "            print ('time elapsed : ' + str((time.time()-start_time)/60) + ' mins')\n",
    "            print('>%s' % name)\n",
    "            print (counter)\n",
    "    return features\n",
    "\n",
    "# extract features from all images\n",
    "directory = 'input/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: train=100, test=100\n",
      "Photos: train=100, test=100\n",
      "Vocabulary Size: 366\n",
      "Description Length: 25\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 7, 7, 512)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 512)          0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 25, 50)       18300       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          65664       global_max_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 25, 256)      314368      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 25, 128)      0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 25, 128)      32896       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 25, 256)      0           repeat_vector_2[0][0]            \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 500)          1514000     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 500)          250500      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 366)          183366      dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,379,094\n",
      "Trainable params: 2,379,094\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      " - 24s - loss: 5.4609 - acc: 0.0850\n",
      "Epoch 2/50\n",
      " - 22s - loss: 5.1562 - acc: 0.1000\n",
      "Epoch 3/50\n",
      " - 22s - loss: 5.1495 - acc: 0.1000\n",
      "Epoch 4/50\n",
      " - 22s - loss: 5.1255 - acc: 0.1000\n",
      "Epoch 5/50\n",
      " - 21s - loss: 5.1015 - acc: 0.0996\n",
      "Epoch 6/50\n",
      " - 20s - loss: 5.0411 - acc: 0.1010\n",
      "Epoch 7/50\n",
      " - 21s - loss: 5.0111 - acc: 0.1030\n",
      "Epoch 8/50\n",
      " - 21s - loss: 4.9638 - acc: 0.1028\n",
      "Epoch 9/50\n",
      " - 20s - loss: 4.9352 - acc: 0.1042\n",
      "Epoch 10/50\n",
      " - 20s - loss: 4.9125 - acc: 0.1035\n",
      "Epoch 11/50\n",
      " - 21s - loss: 4.8856 - acc: 0.1037\n",
      "Epoch 12/50\n",
      " - 24s - loss: 4.8435 - acc: 0.1096\n",
      "Epoch 13/50\n",
      " - 26s - loss: 4.7896 - acc: 0.1141\n",
      "Epoch 14/50\n",
      " - 26s - loss: 4.7491 - acc: 0.1299\n",
      "Epoch 15/50\n",
      " - 25s - loss: 4.7379 - acc: 0.1329\n",
      "Epoch 16/50\n",
      " - 27s - loss: 4.7022 - acc: 0.1375\n",
      "Epoch 17/50\n",
      " - 25s - loss: 4.6398 - acc: 0.1449\n",
      "Epoch 18/50\n",
      " - 22s - loss: 4.5744 - acc: 0.1461\n",
      "Epoch 19/50\n",
      " - 22s - loss: 4.5165 - acc: 0.1513\n",
      "Epoch 20/50\n",
      " - 20s - loss: 4.4698 - acc: 0.1492\n",
      "Epoch 21/50\n",
      " - 24s - loss: 4.4175 - acc: 0.1544\n",
      "Epoch 22/50\n",
      " - 21s - loss: 4.3472 - acc: 0.1576\n",
      "Epoch 23/50\n",
      " - 22s - loss: 4.3179 - acc: 0.1579\n",
      "Epoch 24/50\n",
      " - 32s - loss: 4.2520 - acc: 0.1615\n",
      "Epoch 25/50\n",
      " - 20s - loss: 4.2383 - acc: 0.1556\n",
      "Epoch 26/50\n",
      " - 20s - loss: 4.2465 - acc: 0.1647\n",
      "Epoch 27/50\n",
      " - 20s - loss: 4.2198 - acc: 0.1601\n",
      "Epoch 28/50\n",
      " - 22s - loss: 4.1698 - acc: 0.1601\n",
      "Epoch 29/50\n",
      " - 22s - loss: 4.1029 - acc: 0.1647\n",
      "Epoch 30/50\n",
      " - 23s - loss: 4.0237 - acc: 0.1696\n",
      "Epoch 31/50\n",
      " - 21s - loss: 3.9712 - acc: 0.1706\n",
      "Epoch 32/50\n",
      " - 21s - loss: 3.8990 - acc: 0.1736\n",
      "Epoch 33/50\n",
      " - 24s - loss: 3.8289 - acc: 0.1785\n",
      "Epoch 34/50\n",
      " - 24s - loss: 3.7737 - acc: 0.1746\n",
      "Epoch 35/50\n",
      " - 23s - loss: 3.7163 - acc: 0.1872\n",
      "Epoch 36/50\n",
      " - 20s - loss: 3.6852 - acc: 0.1847\n",
      "Epoch 37/50\n",
      " - 20s - loss: 3.6780 - acc: 0.1791\n",
      "Epoch 38/50\n",
      " - 20s - loss: 3.6899 - acc: 0.1724\n",
      "Epoch 39/50\n",
      " - 20s - loss: 3.6040 - acc: 0.1870\n",
      "Epoch 40/50\n",
      " - 21s - loss: 3.5339 - acc: 0.1836\n",
      "Epoch 41/50\n",
      " - 21s - loss: 3.4666 - acc: 0.2018\n",
      "Epoch 42/50\n",
      " - 25s - loss: 3.4040 - acc: 0.1896\n",
      "Epoch 43/50\n",
      " - 22s - loss: 3.3326 - acc: 0.2075\n",
      "Epoch 44/50\n",
      " - 20s - loss: 3.2556 - acc: 0.2019\n",
      "Epoch 45/50\n",
      " - 22s - loss: 3.1751 - acc: 0.2198\n",
      "Epoch 46/50\n",
      " - 22s - loss: 3.0844 - acc: 0.2218\n",
      "Epoch 47/50\n",
      " - 24s - loss: 3.0249 - acc: 0.2380\n",
      "Epoch 48/50\n",
      " - 24s - loss: 2.9658 - acc: 0.2372\n",
      "Epoch 49/50\n",
      " - 23s - loss: 2.9177 - acc: 0.2417\n",
      "Epoch 50/50\n",
      " - 23s - loss: 2.8877 - acc: 0.2483\n",
      ">1: train=0.039287 test=0.011646\n",
      "          train      test\n",
      "count  1.000000  1.000000\n",
      "mean   0.039287  0.011646\n",
      "std         NaN       NaN\n",
      "min    0.039287  0.011646\n",
      "25%    0.039287  0.011646\n",
      "50%    0.039287  0.011646\n",
      "75%    0.039287  0.011646\n",
      "max    0.039287  0.011646\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from pandas import DataFrame\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pickle import load\n",
    " \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    " \n",
    "# split a dataset into train/test elements\n",
    "def train_test_split(dataset):\n",
    "\t# order keys so the split is consistent\n",
    "\tordered = sorted(dataset)\n",
    "\t# return split dataset as two new sets\n",
    "\treturn set(ordered[:100]), set(ordered[100:200])\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\treturn descriptions\n",
    " \n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    " \n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = list(descriptions.values())\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, desc, image, max_length):\n",
    "\tXimages, XSeq, y = list(), list(),list()\n",
    "\tvocab_size = len(tokenizer.word_index) + 1\n",
    "\t# integer encode the description\n",
    "\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t# split one sequence into multiple X,y pairs\n",
    "\tfor i in range(1, len(seq)):\n",
    "\t\t# select\n",
    "\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t# pad input sequence\n",
    "\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t# encode output sequence\n",
    "\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t# store\n",
    "\t\tXimages.append(image)\n",
    "\t\tXSeq.append(in_seq)\n",
    "\t\ty.append(out_seq)\n",
    "\t# Ximages, XSeq, y = array(Ximages), array(XSeq), array(y)\n",
    "\treturn [Ximages, XSeq, y]\n",
    " \n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor (encoder)\n",
    "\tinputs1 = Input(shape=(7, 7, 512))\n",
    "\tfe1 = GlobalMaxPooling2D()(inputs1)\n",
    "\tfe2 = Dense(128, activation='relu')(fe1)\n",
    "\tfe3 = RepeatVector(max_length)(fe2)\n",
    "\t# embedding\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\temb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
    "\temb3 = LSTM(256, return_sequences=True)(emb2)\n",
    "\temb4 = TimeDistributed(Dense(128, activation='relu'))(emb3)\n",
    "\t# merge inputs\n",
    "\tmerged = concatenate([fe3, emb4])\n",
    "\t# language model (decoder)\n",
    "\tlm2 = LSTM(500)(merged)\n",
    "\tlm3 = Dense(500, activation='relu')(lm2)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tprint(model.summary())\n",
    "# \tplot_model(model, show_shapes=True, to_file='plot.png')\n",
    "\treturn model\n",
    " \n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length, n_step):\n",
    "\t# loop until we finish training\n",
    "\twhile 1:\n",
    "\t\t# loop over photo identifiers in the dataset\n",
    "\t\tkeys = list(descriptions.keys())\n",
    "\t\tfor i in range(0, len(keys), n_step):\n",
    "\t\t\tXimages, XSeq, y = list(), list(),list()\n",
    "\t\t\tfor j in range(i, min(len(keys), i+n_step)):\n",
    "\t\t\t\timage_id = keys[j]\n",
    "\t\t\t\t# retrieve photo feature input\n",
    "\t\t\t\timage = features[image_id][0]\n",
    "\t\t\t\t# retrieve text input\n",
    "\t\t\t\tdesc = descriptions[image_id]\n",
    "\t\t\t\t# generate input-output pairs\n",
    "\t\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, desc, image, max_length)\n",
    "\t\t\t\tfor k in range(len(in_img)):\n",
    "\t\t\t\t\tXimages.append(in_img[k])\n",
    "\t\t\t\t\tXSeq.append(in_seq[k])\n",
    "\t\t\t\t\ty.append(out_word[k])\n",
    "\t\t\t# yield this batch of samples to the model\n",
    "\t\t\tyield [[array(Ximages), array(XSeq)], array(y)]\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "\t# seed the generation process\n",
    "\tin_text = 'startseq'\n",
    "\t# iterate over the whole length of the sequence\n",
    "\tfor i in range(max_length):\n",
    "\t\t# integer encode input sequence\n",
    "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pad input\n",
    "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
    "\t\t# predict next word\n",
    "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
    "\t\t# convert probability to integer\n",
    "\t\tyhat = argmax(yhat)\n",
    "\t\t# map integer to word\n",
    "\t\tword = word_for_id(yhat, tokenizer)\n",
    "\t\t# stop if we cannot map the word\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\t# append as input for generating the next word\n",
    "\t\tin_text += ' ' + word\n",
    "\t\t# stop if we predict the end of the sequence\n",
    "\t\tif word == 'endseq':\n",
    "\t\t\tbreak\n",
    "\treturn in_text\n",
    " \n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "\tactual, predicted = list(), list()\n",
    "\t# step over the whole set\n",
    "\tfor key, desc in descriptions.items():\n",
    "\t\t# generate description\n",
    "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "\t\t# store actual and predicted\n",
    "\t\tactual.append([desc.split()])\n",
    "\t\tpredicted.append(yhat.split())\n",
    "\t# calculate BLEU score\n",
    "\tbleu = corpus_bleu(actual, predicted)\n",
    "\treturn bleu\n",
    " \n",
    "# load dev set\n",
    "filename = 'input/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dataset = load_set(filename)\n",
    "print('Dataset: %d' % len(dataset))\n",
    "# train-test split\n",
    "train, test = train_test_split(dataset)\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max(len(s.split()) for s in list(train_descriptions.values()))\n",
    "print('Description Length: %d' % max_length)\n",
    " \n",
    "# define experiment\n",
    "model_name = 'baseline1'\n",
    "verbose = 2\n",
    "n_epochs = 50\n",
    "n_photos_per_update = 2\n",
    "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
    "n_repeats = 3\n",
    " \n",
    "# run experiment\n",
    "train_results, test_results = list(), list()\n",
    "for i in range(1):\n",
    "\t# define the model\n",
    "\tmodel = define_model(vocab_size, max_length)\n",
    "\t# fit model\n",
    "\tmodel.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
    "\t# evaluate model on training data\n",
    "\ttrain_score = evaluate_model(model, train_descriptions, train_features, tokenizer, max_length)\n",
    "\ttest_score = evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n",
    "\t# store\n",
    "\ttrain_results.append(train_score)\n",
    "\ttest_results.append(test_score)\n",
    "\tprint('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
    "# save results to file\n",
    "df = DataFrame()\n",
    "df['train'] = train_results\n",
    "df['test'] = test_results\n",
    "print(df.describe())\n",
    "df.to_csv(model_name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "\tactual, predicted = list(), list()\n",
    "\t# step over the whole set\n",
    "\tfor key, desc in descriptions.items():\n",
    "\t\t# generate description\n",
    "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "\t\t# store actual and predicted\n",
    "\t\tactual.append([desc.split()])\n",
    "\t\tpredicted.append(yhat.split())\n",
    "\t\tprint('Actual:    %s' % desc)\n",
    "\t\tprint('Predicted: %s' % yhat)\n",
    "\t\tif len(actual) >= 5:\n",
    "\t\t\tbreak\n",
    "\t# calculate BLEU score\n",
    "\tbleu = corpus_bleu(actual, predicted)\n",
    "\treturn bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:    startseq black dog carries an orange tennis ball in his mouth as he swims endseq\n",
      "Predicted: startseq brown dog dog dog in in on ball endseq\n",
      "Actual:    startseq brown dog jumping into pool after bloe ball endseq\n",
      "Predicted: startseq man is is is is is is is is is through his his his his his water endseq\n",
      "Actual:    startseq man is playing saxophone next to fire hydrant endseq\n",
      "Predicted: startseq girl in on on on on front on front on front in front on front on front on front in front in front in front\n",
      "Actual:    startseq man and woman standing outside restaurant endseq\n",
      "Predicted: startseq mother woman and and are in in night endseq\n",
      "Actual:    startseq man in suit along the railing of balcony that has very high view endseq\n",
      "Predicted: startseq man woman fishing in at walk at walk in walk in walk in walk in walk in walk in walk in walk in walk in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "out = evaluate_model(model,train_descriptions,train_features,tokenizer,max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
